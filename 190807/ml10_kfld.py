import pandas as pd
from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import accuracy_score
import warnings
from sklearn.utils.testing import all_estimators

warnings.filterwarnings('ignore')

# 붓꽃 데이터 읽어 들이기
iris_data = pd.read_csv("‪../../Data/iris2.csv", encoding = "UTF-8")

# 붓꽃 데이터를 레이블과 입력 데이터로 분리하기
y = iris_data.loc[ : , "Name"]
x = iris_data.loc[ : , ["SepalLength", "SepalWidth", "PetalLength", "PetalWidth"]]

# classifier 알고리즘 모두 추출하기
warnings.filterwarnings('ignore')
allAlgorithms = all_estimators(type_filter = "classifier")

'''
# K-분할 크로스 밸리데이션 전용 객체
kfold_cv = KFold(n_splits = 5, shuffle = True)

for (name, algorithm) in allAlgorithms:

    # 각 알고리즘 객체 생성하기
    clf = algorithm()

    # scroe 메서드를 가진 클래스를 대상으로 하기
    if hasattr(clf, "score"):

        # 크로스 밸리데이션
        scores = cross_val_score(clf, x, y, cv = kfold_cv)
        print(name, "의 정답률 = ")
        print(scores)
'''

for k in range(3, 11):
    # K-분할 크로스 밸리데이션 전용 객체
    kfold_cv = KFold(n_splits = k, shuffle = True)

    for (name, algorithm) in allAlgorithms:

        # 각 알고리즘 객체 생성하기
        clf = algorithm()

        # scroe 메서드를 가진 클래스를 대상으로 하기
        if hasattr(clf, "score"):

            # 크로스 밸리데이션
            scores = cross_val_score(clf, x, y, cv = kfold_cv)
            print(name, "의 정답률 = ")
            print(scores)

# n_split = 3
# AdaBoostClassifier 의 정답률 =
# [0.94 1.   0.92]
# BaggingClassifier 의 정답률 =
# [0.96 0.96 0.96]
# BernoulliNB 의 정답률 =
# [0.28 0.28 0.32]
# CalibratedClassifierCV 의 정답률 =
# [0.96 0.94 0.88]
# ComplementNB 의 정답률 =
# [0.72 0.64 0.64]
# DecisionTreeClassifier 의 정답률 =
# [0.94 0.92 1.  ]
# ExtraTreeClassifier 의 정답률 =
# [0.96 0.94 0.92]
# ExtraTreesClassifier 의 정답률 =
# [0.94 1.   0.94]
# GaussianNB 의 정답률 =
# [0.94 1.   0.9 ]
# GaussianProcessClassifier 의 정답률 =
# [0.98 0.92 0.96]
# GradientBoostingClassifier 의 정답률 =
# [0.98 0.98 0.94]
# KNeighborsClassifier 의 정답률 =
# [0.98 1.   0.98]
# LabelPropagation 의 정답률 =
# [0.92 0.98 0.96]
# LabelSpreading 의 정답률 =
# [0.96 0.96 0.98]
# LinearDiscriminantAnalysis 의 정답률 =
# [0.98 0.98 0.98]
# LinearSVC 의 정답률 =
# [0.96 0.9  0.94]
# LogisticRegression 의 정답률 =
# [0.98 0.98 0.9 ]
# LogisticRegressionCV 의 정답률 =
# [0.92 0.98 0.96]
# MLPClassifier 의 정답률 =
# [0.96 0.94 0.98]
# MultinomialNB 의 정답률 =
# [0.96 0.64 0.58]
# NearestCentroid 의 정답률 =
# [0.96 0.9  0.92]
# NuSVC 의 정답률 =
# [0.92 0.94 0.94]
# PassiveAggressiveClassifier 의 정답률 =
# [0.7  0.96 0.76]
# Perceptron 의 정답률 =
# [0.66 0.82 0.7 ]
# QuadraticDiscriminantAnalysis 의 정답률 =
# [1.   0.92 0.96]
# RadiusNeighborsClassifier 의 정답률 =
# [0.94 0.98 0.98]
# RandomForestClassifier 의 정답률 =
# [0.92 0.96 0.92]
# RidgeClassifier 의 정답률 =
# [0.84 0.78 0.82]
# RidgeClassifierCV 의 정답률 =
# [0.82 0.78 0.9 ]
# SGDClassifier 의 정답률 =
# [0.7  0.68 0.68]
# SVC 의 정답률 =
# [0.94 1.   0.94]

# n_split = 4
# AdaBoostClassifier 의 정답률 =
# [0.89473684 0.92105263 0.97297297 0.91891892]
# BaggingClassifier 의 정답률 =
# [0.89473684 0.92105263 0.94594595 0.97297297]
# BernoulliNB 의 정답률 =
# [0.28947368 0.28947368 0.27027027 0.13513514]
# CalibratedClassifierCV 의 정답률 =
# [0.92105263 1.         0.86486486 0.97297297]
# ComplementNB 의 정답률 =
# [0.68421053 0.63157895 0.67567568 0.67567568]
# DecisionTreeClassifier 의 정답률 =
# [0.92105263 0.94736842 0.94594595 0.97297297]
# ExtraTreeClassifier 의 정답률 =
# [0.86842105 0.86842105 1.         0.81081081]
# ExtraTreesClassifier 의 정답률 =
# [0.92105263 1.         0.91891892 0.94594595]
# GaussianNB 의 정답률 =
# [0.92105263 0.94736842 0.94594595 0.97297297]
# GaussianProcessClassifier 의 정답률 =
# [0.94736842 0.97368421 0.94594595 1.        ]
# GradientBoostingClassifier 의 정답률 =
# [0.94736842 0.92105263 0.97297297 0.97297297]
# KNeighborsClassifier 의 정답률 =
# [0.97368421 1.         0.97297297 0.91891892]
# LabelPropagation 의 정답률 =
# [0.97368421 1.         0.91891892 0.94594595]
# LabelSpreading 의 정답률 =
# [1.         0.97368421 0.91891892 0.91891892]
# LinearDiscriminantAnalysis 의 정답률 =
# [0.94736842 0.97368421 1.         1.        ]
# LinearSVC 의 정답률 =
# [0.94736842 1.         0.91891892 0.97297297]
# LogisticRegression 의 정답률 =
# [0.97368421 0.94736842 1.         0.89189189]
# LogisticRegressionCV 의 정답률 =
# [0.97368421 0.92105263 0.97297297 0.89189189]
# MLPClassifier 의 정답률 =
# [1.         0.97368421 0.97297297 0.97297297]
# MultinomialNB 의 정답률 =
# [0.65789474 0.97368421 0.83783784 0.83783784]
# NearestCentroid 의 정답률 =
# [0.86842105 0.97368421 0.91891892 0.94594595]
# NuSVC 의 정답률 =
# [0.92105263 0.97368421 0.94594595 1.        ]
# PassiveAggressiveClassifier 의 정답률 =
# [0.86842105 0.89473684 0.64864865 0.94594595]
# Perceptron 의 정답률 =
# [0.65789474 0.60526316 0.75675676 0.51351351]
# QuadraticDiscriminantAnalysis 의 정답률 =
# [0.92105263 1.         0.97297297 0.97297297]
# RadiusNeighborsClassifier 의 정답률 =
# [0.97368421 0.84210526 0.97297297 0.94594595]
# RandomForestClassifier 의 정답률 =
# [0.97368421 0.89473684 1.         0.94594595]
# RidgeClassifier 의 정답률 =
# [0.89473684 0.86842105 0.86486486 0.81081081]
# RidgeClassifierCV 의 정답률 =
# [0.86842105 0.76315789 0.83783784 0.83783784]
# SGDClassifier 의 정답률 =
# [0.55263158 0.89473684 0.62162162 0.7027027 ]
# SVC 의 정답률 =
# [0.97368421 0.92105263 0.94594595 1.        ]

# n_split = 5
# AdaBoostClassifier 의 정답률 =
# [0.9        0.9        0.96666667 0.96666667 0.93333333]
# BaggingClassifier 의 정답률 =
# [0.96666667 0.96666667 0.96666667 1.         0.93333333]
# BernoulliNB 의 정답률 =
# [0.26666667 0.26666667 0.26666667 0.3        0.26666667]
# CalibratedClassifierCV 의 정답률 =
# [0.96666667 0.9        0.96666667 0.96666667 0.93333333]
# ComplementNB 의 정답률 =
# [0.7        0.63333333 0.73333333 0.6        0.66666667]
# DecisionTreeClassifier 의 정답률 =
# [0.96666667 0.96666667 0.93333333 0.93333333 0.93333333]
# ExtraTreeClassifier 의 정답률 =
# [0.93333333 0.9        0.83333333 0.93333333 0.93333333]
# ExtraTreesClassifier 의 정답률 =
# [0.9        0.96666667 0.93333333 0.96666667 0.96666667]
# GaussianNB 의 정답률 =
# [0.93333333 0.93333333 0.9        1.         0.96666667]
# GaussianProcessClassifier 의 정답률 =
# [0.93333333 0.9        0.96666667 0.96666667 1.        ]
# GradientBoostingClassifier 의 정답률 =
# [0.9        0.93333333 0.93333333 1.         1.        ]
# KNeighborsClassifier 의 정답률 =
# [1.         1.         0.96666667 0.96666667 0.93333333]
# LabelPropagation 의 정답률 =
# [0.93333333 1.         0.96666667 0.93333333 0.96666667]
# LabelSpreading 의 정답률 =
# [0.96666667 0.93333333 1.         0.93333333 0.96666667]
# LinearDiscriminantAnalysis 의 정답률 =
# [0.96666667 0.96666667 0.96666667 1.         1.        ]
# LinearSVC 의 정답률 =
# [0.93333333 0.86666667 0.96666667 1.         0.96666667]
# LogisticRegression 의 정답률 =
# [0.96666667 0.93333333 1.         0.9        0.96666667]
# LogisticRegressionCV 의 정답률 =
# [0.96666667 0.9        0.83333333 1.         1.        ]
# MLPClassifier 의 정답률 =
# [0.93333333 1.         0.96666667 0.96666667 0.96666667]
# MultinomialNB 의 정답률 =
# [0.9        0.6        0.93333333 0.9        0.6       ]
# NearestCentroid 의 정답률 =
# [0.8        1.         0.93333333 0.86666667 0.96666667]
# NuSVC 의 정답률 =
# [0.96666667 0.93333333 1.         0.93333333 0.96666667]
# PassiveAggressiveClassifier 의 정답률 =
# [0.8        0.66666667 0.53333333 0.56666667 0.73333333]
# Perceptron 의 정답률 =
# [0.6        0.6        0.4        0.7        0.53333333]
# QuadraticDiscriminantAnalysis 의 정답률 =
# [0.96666667 1.         0.93333333 0.96666667 1.        ]
# RadiusNeighborsClassifier 의 정답률 =
# [0.86666667 1.         0.93333333 0.93333333 1.        ]
# RandomForestClassifier 의 정답률 =
# [0.86666667 0.96666667 0.96666667 1.         0.96666667]
# RidgeClassifier 의 정답률 =
# [0.83333333 0.8        0.76666667 0.93333333 0.83333333]
# RidgeClassifierCV 의 정답률 =
# [0.9        0.8        0.83333333 0.9        0.76666667]
# SGDClassifier 의 정답률 =
# [0.63333333 0.66666667 0.7        1.         0.66666667]
# SVC 의 정답률 =
# [0.96666667 0.96666667 1.         0.96666667 1.        ]

# n_split = 6
# AdaBoostClassifier 의 정답률 =
# [0.96 0.88 1.   0.96 0.96 0.96]
# BaggingClassifier 의 정답률 =
# [0.88 1.   1.   0.92 0.96 1.  ]
# BernoulliNB 의 정답률 =
# [0.28 0.2  0.24 0.28 0.24 0.24]
# CalibratedClassifierCV 의 정답률 =
# [0.88 1.   0.8  0.92 0.92 1.  ]
# ComplementNB 의 정답률 =
# [0.52 0.68 0.72 0.68 0.68 0.72]
# DecisionTreeClassifier 의 정답률 =
# [0.96 0.88 1.   1.   0.92 0.96]
# ExtraTreeClassifier 의 정답률 =
# [0.88 0.8  0.96 0.96 0.92 1.  ]
# ExtraTreesClassifier 의 정답률 =
# [0.92 1.   0.96 0.96 0.96 0.96]
# GaussianNB 의 정답률 =
# [0.96 1.   0.84 0.96 0.92 1.  ]
# GaussianProcessClassifier 의 정답률 =
# [0.88 0.96 1.   0.96 0.96 1.  ]
# GradientBoostingClassifier 의 정답률 =
# [0.92 0.96 1.   0.92 1.   0.84]
# KNeighborsClassifier 의 정답률 =
# [0.96 1.   0.96 0.96 0.96 0.96]
# LabelPropagation 의 정답률 =
# [0.88 0.92 0.96 1.   1.   1.  ]
# LabelSpreading 의 정답률 =
# [0.96 0.96 0.96 0.96 0.96 0.96]
# LinearDiscriminantAnalysis 의 정답률 =
# [1.   0.96 0.92 0.96 1.   1.  ]
# LinearSVC 의 정답률 =
# [0.96 1.   0.96 1.   0.92 0.96]
# LogisticRegression 의 정답률 =
# [0.92 0.92 1.   0.88 0.92 1.  ]
# LogisticRegressionCV 의 정답률 =
# [0.96 1.   0.8  0.92 0.96 1.  ]
# MLPClassifier 의 정답률 =
# [0.88 0.96 0.96 1.   1.   0.92]
# MultinomialNB 의 정답률 =
# [0.64 1.   0.92 0.76 0.84 0.8 ]
# NearestCentroid 의 정답률 =
# [0.88 0.88 0.92 1.   1.   0.92]
# NuSVC 의 정답률 =
# [0.96 1.   0.96 0.92 0.96 1.  ]
# PassiveAggressiveClassifier 의 정답률 =
# [0.72 0.6  0.88 0.96 0.56 0.64]
# Perceptron 의 정답률 =
# [0.68 0.68 0.56 0.24 0.64 0.76]
# QuadraticDiscriminantAnalysis 의 정답률 =
# [1.   0.96 0.96 0.96 0.96 1.  ]
# RadiusNeighborsClassifier 의 정답률 =
# [0.96 0.96 1.   0.96 0.92 0.92]
# RandomForestClassifier 의 정답률 =
# [1.   0.92 0.96 0.96 0.92 0.92]
# RidgeClassifier 의 정답률 =
# [0.84 0.92 0.76 0.8  0.84 0.76]
# RidgeClassifierCV 의 정답률 =
# [0.88 0.76 0.84 0.8  0.76 0.92]
# SGDClassifier 의 정답률 =
# [0.44 0.88 0.92 0.68 0.92 0.68]
# SVC 의 정답률 =
# [1.   0.92 1.   1.   0.96 1.  ]

# n_split = 7
# AdaBoostClassifier 의 정답률 =
# [0.95454545 0.90909091 0.90909091 1.         0.95238095 1. 0.95238095]
# BaggingClassifier 의 정답률 =
# [0.90909091 1.         0.95454545 0.95238095 0.95238095 0.95238095 1.        ]
# BernoulliNB 의 정답률 =
# [0.18181818 0.27272727 0.22727273 0.28571429 0.28571429 0.19047619 0.23809524]
# CalibratedClassifierCV 의 정답률 =
# [0.90909091 0.95454545 0.95454545 0.95238095 0.95238095 0.9047619  0.9047619 ]
# ComplementNB 의 정답률 =
# [0.68181818 0.5        0.72727273 0.71428571 0.76190476 0.52380952 0.76190476]
# DecisionTreeClassifier 의 정답률 =
# [0.90909091 0.95454545 0.90909091 0.95238095 1.         1.         0.95238095]
# ExtraTreeClassifier 의 정답률 =
# [1.         0.90909091 0.90909091 0.9047619  0.95238095 0.9047619  0.95238095]
# ExtraTreesClassifier 의 정답률 =
# [1.         1.         0.90909091 0.9047619  1.         0.95238095 0.95238095]
# GaussianNB 의 정답률 =
# [1.         0.95454545 0.95454545 1.         0.95238095 0.85714286 0.95238095]
# GaussianProcessClassifier 의 정답률 =
# [0.95454545 1.         0.95454545 0.9047619  1.         0.95238095 0.95238095]
# GradientBoostingClassifier 의 정답률 =
# [0.95454545 1.         0.90909091 0.9047619  0.95238095 0.95238095 1.        ]
# KNeighborsClassifier 의 정답률 =
# [1.         1.         0.95454545 1.         1.         0.85714286 1.        ]
# LabelPropagation 의 정답률 =
# [0.90909091 0.90909091 1.         0.95238095 1.         1.         0.95238095]
# LabelSpreading 의 정답률 =
# [0.95454545 0.95454545 0.95454545 0.95238095 1.         0.9047619  1.        ]
# LinearDiscriminantAnalysis 의 정답률 =
# [1.         0.90909091 1.         1.         1.         0.95238095 1.        ]
# LinearSVC 의 정답률 =
# [0.95454545 0.95454545 0.95454545 0.9047619  0.95238095 1.         1.        ]
# LogisticRegression 의 정답률 =
# [0.95454545 1.         1.         0.80952381 1.         0.9047619  1.        ]
# LogisticRegressionCV 의 정답률 =
# [0.95454545 0.90909091 0.86363636 0.95238095 0.66666667 0.95238095 1.        ]
# MLPClassifier 의 정답률 =
# [0.90909091 1.         0.95454545 1.         1.         1.         0.9047619 ]
# MultinomialNB 의 정답률 =
# [0.59090909 0.77272727 1.         1.         1.         0.9047619  0.66666667]
# NearestCentroid 의 정답률 =
# [0.95454545 0.90909091 0.90909091 0.95238095 0.85714286 0.9047619  0.9047619 ]
# NuSVC 의 정답률 =
# [0.95454545 0.90909091 1.         0.95238095 0.95238095 0.95238095  0.95238095]
# PassiveAggressiveClassifier 의 정답률 =
# [0.72727273 0.68181818 0.86363636 0.71428571 0.52380952 0.80952381  0.9047619 ]
# Perceptron 의 정답률 =
# [0.72727273 0.45454545 0.81818182 0.47619048 0.66666667 0.47619048  0.85714286]
# QuadraticDiscriminantAnalysis 의 정답률 =
# [0.95454545 0.95454545 1.         0.95238095 1.         1.          1.        ]
# RadiusNeighborsClassifier 의 정답률 =
# [1.         0.86363636 0.95454545 1.         0.95238095 0.9047619   0.95238095]
# RandomForestClassifier 의 정답률 =
# [0.81818182 0.90909091 0.95454545 1.         1.         1.          0.9047619 ]
# RidgeClassifier 의 정답률 =
# [0.86363636 0.95454545 0.86363636 0.85714286 0.76190476 0.76190476  0.85714286]
# RidgeClassifierCV 의 정답률 =
# [0.77272727 0.81818182 0.77272727 0.80952381 0.76190476 0.95238095  0.80952381]
# SGDClassifier 의 정답률 =
# [0.77272727 0.95454545 0.63636364 0.66666667 0.76190476 0.57142857  0.76190476]
# SVC 의 정답률 =
# [0.95454545 1.         1.         1.         1.         0.9047619   0.95238095]

# n_split = 8
# AdaBoostClassifier 의 정답률 =
# [0.89473684 0.94736842 0.94736842 0.89473684 0.94736842 0.94736842  1.         1.        ]
# BaggingClassifier 의 정답률 =
# [0.94736842 1.         0.89473684 0.94736842 0.94736842 0.94736842  0.94444444 1.        ]
# BernoulliNB 의 정답률 =
# [0.21052632 0.15789474 0.26315789 0.21052632 0.15789474 0.26315789  0.22222222 0.16666667]
# CalibratedClassifierCV 의 정답률 =
# [0.89473684 0.94736842 0.89473684 1.         1.         0.89473684  0.83333333 0.94444444]
# ComplementNB 의 정답률 =
# [0.68421053 0.73684211 0.73684211 0.57894737 0.63157895 0.57894737  0.72222222 0.66666667]
# DecisionTreeClassifier 의 정답률 =
# [0.89473684 1.         0.84210526 0.94736842 1.         0.94736842  1.         1.        ]
# ExtraTreeClassifier 의 정답률 =
# [1.         1.         0.89473684 0.73684211 0.94736842 1.          0.94444444 1.        ]
# ExtraTreesClassifier 의 정답률 =
# [0.94736842 0.94736842 1.         0.89473684 0.94736842 0.89473684  1.         1.        ]
# GaussianNB 의 정답률 =
# [0.89473684 0.89473684 1.         1.         1.         1.          0.94444444 0.94444444]
# GaussianProcessClassifier 의 정답률 =
# [0.94736842 1.         1.         0.89473684 0.89473684 1.          0.94444444 1.        ]
# GradientBoostingClassifier 의 정답률 =
# [0.94736842 0.94736842 1.         0.94736842 0.94736842 0.89473684  0.94444444 1.        ]
# KNeighborsClassifier 의 정답률 =
# [1.         0.94736842 0.94736842 1.         0.89473684 1.          1.         1.        ]
# LabelPropagation 의 정답률 =
# [0.94736842 1.         0.94736842 1.         0.89473684 1.          1.         0.88888889]
# LabelSpreading 의 정답률 =
# [1.         0.94736842 1.         1.         0.94736842 0.94736842  0.83333333 1.        ]
# LinearDiscriminantAnalysis 의 정답률 =
# [0.94736842 0.94736842 1.         1.         1.         0.94736842  1.         1.        ]
# LinearSVC 의 정답률 =
# [1.         1.         0.94736842 0.84210526 0.94736842 0.89473684  1.         1.        ]
# LogisticRegression 의 정답률 =
# [1.         0.94736842 1.         0.89473684 0.94736842 0.94736842  0.94444444 0.94444444]
# LogisticRegressionCV 의 정답률 =
# [0.78947368 0.89473684 0.89473684 0.94736842 0.94736842 1.          0.88888889 0.94444444]
# MLPClassifier 의 정답률 =
# [1.         0.84210526 1.         1.         1.         1.          1.         0.88888889]
# MultinomialNB 의 정답률 =
# [0.94736842 0.94736842 0.94736842 1.         0.94736842 0.78947368  0.94444444 0.94444444]
# NearestCentroid 의 정답률 =
# [0.89473684 1.         0.89473684 0.89473684 1.         1.          1.         0.72222222]
# NuSVC 의 정답률 =
# [0.94736842 1.         0.94736842 0.89473684 0.94736842 1.          1.         0.94444444]
# PassiveAggressiveClassifier 의 정답률 =
# [0.73684211 0.78947368 0.73684211 0.84210526 0.57894737 0.68421053  0.83333333 0.83333333]
# Perceptron 의 정답률 =
# [0.57894737 0.47368421 0.68421053 0.57894737 0.89473684 0.89473684  1.         0.77777778]
# QuadraticDiscriminantAnalysis 의 정답률 =
# [1.         0.94736842 0.94736842 1.         0.94736842 0.94736842  1.         1.        ]
# RadiusNeighborsClassifier 의 정답률 =
# [1.         0.94736842 0.94736842 1.         0.94736842 0.94736842  0.94444444 0.83333333]
# RandomForestClassifier 의 정답률 =
# [0.89473684 0.94736842 0.94736842 0.84210526 0.73684211 1.          1.         1.        ]
# RidgeClassifier 의 정답률 =
# [0.84210526 0.89473684 0.73684211 0.94736842 0.89473684 0.73684211  0.94444444 0.72222222]
# RidgeClassifierCV 의 정답률 =
# [1.         0.68421053 0.78947368 0.84210526 0.89473684 1.          0.72222222 0.83333333]
# SGDClassifier 의 정답률 =
# [0.94736842 0.84210526 0.63157895 0.52631579 0.89473684 0.73684211  0.5        0.66666667]
# SVC 의 정답률 =
# [0.94736842 1.         0.94736842 1.         1.         1.          1.         1.        ]

# n_split = 9
# AdaBoostClassifier 의 정답률 =
# [0.88235294 0.94117647 0.94117647 0.94117647 0.76470588 1.          0.9375     1.         1.        ]
# BaggingClassifier 의 정답률 =
# [0.88235294 1.         1.         1.         1.         0.88235294  0.875      0.9375     1.        ]
# BernoulliNB 의 정답률 =
# [0.23529412 0.17647059 0.29411765 0.23529412 0.23529412 0.17647059  0.25       0.125      0.25      ]
# CalibratedClassifierCV 의 정답률 =
# [0.88235294 1.         1.         0.94117647 0.88235294 1.          0.8125     1.         0.9375    ]
# ComplementNB 의 정답률 =
# [0.82352941 0.64705882 0.58823529 0.76470588 0.64705882 0.82352941  0.5        0.5625     0.625     ]
# DecisionTreeClassifier 의 정답률 =
# [0.88235294 0.94117647 0.88235294 1.         0.94117647 1.          1.         0.9375     1.        ]
# ExtraTreeClassifier 의 정답률 =
# [0.94117647 0.82352941 0.94117647 0.94117647 0.94117647 0.94117647  0.8125     0.9375     0.9375    ]
# ExtraTreesClassifier 의 정답률 =
# [0.94117647 1.         0.82352941 0.94117647 1.         1.          0.9375     0.9375     1.        ]
# GaussianNB 의 정답률 =
# [1.         1.         0.76470588 1.         0.94117647 1.          0.9375     1.         0.9375    ]
# GaussianProcessClassifier 의 정답률 =
# [0.94117647 1.         1.         0.94117647 0.94117647 0.94117647  0.9375     1.         0.875     ]
# GradientBoostingClassifier 의 정답률 =
# [0.94117647 0.94117647 0.94117647 1.         1.         1.          0.9375     0.9375     0.875     ]
# KNeighborsClassifier 의 정답률 =
# [1.         0.94117647 0.94117647 0.94117647 1.         0.94117647  1.         0.9375     1.        ]
# LabelPropagation 의 정답률 =
# [0.94117647 1.         1.         0.94117647 1.         0.94117647  0.875      1.         1.        ]
# LabelSpreading 의 정답률 =
# [1.         0.94117647 1.         0.88235294 1.         0.94117647  1.         0.9375     0.9375    ]
# LinearDiscriminantAnalysis 의 정답률 =
# [1.         1.         0.94117647 1.         0.94117647 1.          0.9375     1.         1.        ]
# LinearSVC 의 정답률 =
# [0.76470588 0.94117647 1.         0.94117647 0.94117647 1.          1.         1.         0.9375    ]
# LogisticRegression 의 정답률 =
# [1.         1.         1.         0.94117647 0.82352941 1.          0.875      0.9375     1.        ]
# LogisticRegressionCV 의 정답률 =
# [0.94117647 0.88235294 0.88235294 0.88235294 0.82352941 0.88235294  1.         1.         0.875     ]
# MLPClassifier 의 정답률 =
# [0.94117647 0.94117647 0.94117647 1.         0.94117647 1.          1.         1.         1.        ]
# MultinomialNB 의 정답률 =
# [1.         0.70588235 0.64705882 0.76470588 0.64705882 1.          1.         0.8125     0.875     ]
# NearestCentroid 의 정답률 =
# [0.88235294 0.94117647 1.         0.88235294 0.94117647 0.82352941  0.9375     0.9375     0.9375    ]
# NuSVC 의 정답률 =
# [1.         0.88235294 0.94117647 1.         1.         1.          0.9375     0.875      1.        ]
# PassiveAggressiveClassifier 의 정답률 =
# [0.64705882 0.70588235 0.94117647 0.76470588 0.58823529 0.70588235  0.6875     0.75       0.375     ]
# Perceptron 의 정답률 =
# [0.88235294 0.82352941 0.88235294 0.94117647 0.64705882 0.70588235  0.625      0.9375     0.6875    ]
# QuadraticDiscriminantAnalysis 의 정답률 =
# [0.88235294 0.94117647 1.         1.         1.         1.          0.9375     1.         1.        ]
# RadiusNeighborsClassifier 의 정답률 =
# [1.         0.94117647 1.         0.88235294 0.88235294 0.94117647  1.         1.         0.9375    ]
# RandomForestClassifier 의 정답률 =
# [0.88235294 0.94117647 1.         0.88235294 1.         1.          0.9375     0.875      0.9375    ]
# RidgeClassifier 의 정답률 =
# [0.88235294 0.82352941 0.88235294 0.88235294 0.94117647 0.70588235  0.8125     0.8125     0.75      ]
# RidgeClassifierCV 의 정답률 =
# [0.88235294 1.         0.82352941 0.88235294 0.88235294 0.82352941  0.875      0.6875     0.6875    ]
# SGDClassifier 의 정답률 =
# [0.76470588 0.58823529 0.94117647 0.64705882 0.52941176 0.64705882  0.625      0.5625     0.875     ]
# SVC 의 정답률 =
# [1.         1.         0.94117647 1.         1.         0.94117647  1.         1.         1.        ]

# n_split = 10
# AdaBoostClassifier 의 정답률 =
# [0.93333333 1.         0.93333333 0.93333333 1.         1.          0.86666667 1.         0.8        0.8       ]
# BaggingClassifier 의 정답률 =
# [0.93333333 1.         0.93333333 1.         0.93333333 1.          1.         0.93333333 0.86666667 0.86666667]
# BernoulliNB 의 정답률 =
# [0.26666667 0.13333333 0.26666667 0.13333333 0.26666667 0.26666667  0.2        0.26666667 0.26666667 0.2       ]
# CalibratedClassifierCV 의 정답률 =
# [0.86666667 0.86666667 0.93333333 1.         0.93333333 0.93333333  1.         1.         0.93333333 0.86666667]
# ComplementNB 의 정답률 =
# [0.66666667 0.6        0.66666667 0.66666667 0.6        0.8         0.66666667 0.66666667 0.66666667 0.66666667]
# DecisionTreeClassifier 의 정답률 =
# [1.         0.8        0.93333333 0.93333333 0.93333333 1.          0.93333333 1.         0.93333333 1.        ]
# ExtraTreeClassifier 의 정답률 =
# [0.93333333 0.8        0.93333333 0.93333333 0.93333333 1.          0.93333333 1.         1.         0.93333333]
# ExtraTreesClassifier 의 정답률 =
# [1.         1.         1.         1.         0.86666667 1.          0.86666667 0.86666667 1.         0.93333333]
# GaussianNB 의 정답률 =
# [1.         0.86666667 1.         1.         0.93333333 0.93333333  1.         0.93333333 0.93333333 1.        ]
# GaussianProcessClassifier 의 정답률 =
# [1.         0.93333333 1.         0.93333333 0.93333333 0.86666667  1.         1.         0.93333333 1.        ]
# GradientBoostingClassifier 의 정답률 =
# [1.         1.         0.93333333 1.         0.86666667 0.93333333  0.93333333 0.93333333 1.         1.        ]
# KNeighborsClassifier 의 정답률 =
# [1.         0.93333333 1.         0.93333333 0.93333333 0.93333333  1.         1.         1.         0.93333333]
# LabelPropagation 의 정답률 =
# [0.93333333 0.93333333 1.         1.         0.86666667 0.86666667  1.         0.93333333 1.         1.        ]
# LabelSpreading 의 정답률 =
# [1.         1.         1.         0.93333333 0.86666667 1.          0.93333333 1.         0.93333333 0.93333333]
# LinearDiscriminantAnalysis 의 정답률 =
# [1.         1.         1.         1.         0.93333333 0.93333333  1.         0.93333333 1.         1.        ]
# LinearSVC 의 정답률 =
# [0.93333333 1.         0.93333333 1.         0.93333333 0.93333333  0.86666667 0.93333333 1.         1.        ]
# LogisticRegression 의 정답률 =
# [0.86666667 1.         0.93333333 1.         0.93333333 1.          0.93333333 1.         1.         0.86666667]
# LogisticRegressionCV 의 정답률 =
# [0.86666667 1.         0.93333333 0.93333333 1.         0.93333333  0.8        0.93333333 0.93333333 1.        ]
# MLPClassifier 의 정답률 =
# [1.         1.         0.93333333 1.         1.         0.93333333  1.         1.         1.         0.93333333]
# MultinomialNB 의 정답률 =
# [0.8        0.73333333 0.93333333 0.86666667 0.93333333 0.86666667  0.8        1.         0.93333333 0.33333333]
# NearestCentroid 의 정답률 =
# [1.         0.93333333 1.         0.93333333 1.         1.          0.86666667 0.73333333 0.86666667 0.93333333]
# NuSVC 의 정답률 =
# [0.93333333 1.         1.         0.86666667 1.         1.          0.86666667 0.86666667 1.         1.        ]
# PassiveAggressiveClassifier 의 정답률 =
# [0.93333333 0.66666667 0.86666667 1.         0.66666667 0.6         0.73333333 0.86666667 0.8        0.66666667]
# Perceptron 의 정답률 =
# [1.         0.4        0.26666667 0.73333333 0.66666667 0.46666667  0.6        0.73333333 1.         0.66666667]
# QuadraticDiscriminantAnalysis 의 정답률 =
# [1.         1.         1.         0.93333333 0.93333333 0.86666667  1.         1.         1.         1.        ]
# RadiusNeighborsClassifier 의 정답률 =
# [0.93333333 0.93333333 1.         1.         0.8        0.93333333  1.         0.93333333 0.8        0.93333333]
# RandomForestClassifier 의 정답률 =
# [1.         0.86666667 1.         0.93333333 1.         1.          0.86666667 0.93333333 1.         0.93333333]
# RidgeClassifier 의 정답률 =
# [0.73333333 0.93333333 0.93333333 0.66666667 0.8        0.66666667  0.93333333 0.86666667 0.86666667 0.93333333]
# RidgeClassifierCV 의 정답률 =
# [0.73333333 0.93333333 0.86666667 0.73333333 0.86666667 0.66666667  0.93333333 0.73333333 0.86666667 0.93333333]
# SGDClassifier 의 정답률 =
# [0.6        0.73333333 0.73333333 0.86666667 0.8        0.73333333  0.6        0.33333333 0.4        0.73333333]
# SVC 의 정답률 =
# [1.         1.         1.         1.         0.86666667 1.          0.93333333 0.93333333 1.         1.        ]